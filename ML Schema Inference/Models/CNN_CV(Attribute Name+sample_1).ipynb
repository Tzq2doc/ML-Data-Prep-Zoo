{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate,GlobalMaxPooling1D\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "np.random.seed(512)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import LeakyReLU, BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 128\n",
    "maxlen = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (2,5,10,11,12,13,14,15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "dict_label = {'Usable directly numeric':0, 'Usable with extraction':1, 'Usable with Extration': 1, 'Usable with extraction ':1, 'Usable directly categorical':2, 'Unusable':3, 'Context_specific':4, 'Usable directly categorical ':2}\n",
    "data = pd.read_csv('data_for_ML_num.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]\n",
    "data_LSTM = pd.concat([data['Attribute_name'], data['sample_1'], data['sample_2'], data['sample_3'], data['sample_4'], data['sample_5']], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hghj\n",
      "hghj\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Num of nans': 'Num_of_nans', 'num of dist_val': 'num_of_dist_val'})\n",
    "\n",
    "# data['Num_of_nans'] = [float(data['Num_of_nans'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "# data['num_of_dist_val'] = [float(data['num_of_dist_val'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "\n",
    "data1 = data[['Num_of_nans', 'max_val', 'mean', 'min_val', 'num_of_dist_val','std_dev','castability','extractability', 'len_val','Total_val']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1['Num_of_nans'] = [data1['Num_of_nans'][i]*100/data1['Total_val'][i] for i in data1.index]\n",
    "data1['num_of_dist_val'] = [(data1['num_of_dist_val'][i]*1.0)/data1['Total_val'][i] for i in data1.index]\n",
    "\n",
    "print('hghj')\n",
    "data1 = data1.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev'})\n",
    "data1.loc[data1['scaled_min_val'] > 10000, 'scaled_min_val'] = 10000\n",
    "data1.loc[data1['scaled_min_val'] < -10000, 'scaled_min_val'] = -10000\n",
    "data1.loc[data1['scaled_max_val'] > 10000, 'scaled_max_val'] = 10000\n",
    "data1.loc[data1['scaled_max_val'] < -10000, 'scaled_max_val'] = -10000\n",
    "data1.loc[data1['scaled_mean'] > 10000, 'scaled_mean'] = 10000\n",
    "data1.loc[data1['scaled_mean'] < -10000, 'scaled_mean'] = -10000\n",
    "data1.loc[data1['scaled_std_dev'] > 10000, 'scaled_std_dev'] = 10000\n",
    "data1.loc[data1['scaled_std_dev'] < -10000, 'scaled_std_dev'] = -10000\n",
    "column_names_to_normalize = ['scaled_max_val', 'scaled_mean', 'scaled_min_val','scaled_std_dev','num_of_dist_val','Num_of_nans','Total_val']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "data1.Num_of_nans = data1.Num_of_nans.astype(float)\n",
    "data1.num_of_dist_val = data1.num_of_dist_val.astype(float)\n",
    "data1.castability = data1.castability.astype(float)\n",
    "data1.extractability = data1.extractability.astype(float)\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "\n",
    "print('hghj')\n",
    "\n",
    "data1 = data1[['Num_of_nans', 'scaled_max_val', 'scaled_mean', 'scaled_min_val', 'num_of_dist_val','scaled_std_dev','castability','extractability', 'len_val','Total_val']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test,y_train,y_test = train_test_split(data2,y, test_size=0.2,random_state=100)\n",
    "# key_name = data['Attribute_name']\n",
    "# atr_train,atr_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "\n",
    "# # print(X_train)\n",
    "# X_train.to_csv('xtrain.csv')\n",
    "# atr_train.to_csv('attrain.csv')\n",
    "\n",
    "# new_df  = X_train\n",
    "# X_train = X_train.reset_index(drop=True)\n",
    "# atr_train.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# X_train.to_csv('xtrain1.csv')\n",
    "# X_test = X_test.reset_index(drop=True)\n",
    "# y_train = y_train.reset_index(drop=True)\n",
    "# y_test = y_test.reset_index(drop=True)\n",
    "# atr_train.reset_index(inplace=True,drop=True)\n",
    "# atr_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# lendf = len(X_train)\n",
    "# print(len(X_train))\n",
    "# # print(X_train)\n",
    "\n",
    "# X_trainv = X_train.values\n",
    "# y_trainv = y_train.values\n",
    "# atr_trainv = atr_train.values\n",
    "\n",
    "\n",
    "# print(X_trainv)\n",
    "# print(atr_trainv)\n",
    "\n",
    "# # i=0\n",
    "# # for index,row in X_train.iterrows():\n",
    "# #     print(index)\n",
    "# #     if i==3:\n",
    "# #         break\n",
    "# #     print(row)\n",
    "# #     new_df.loc[lendf+i] = row\n",
    "# #     i=i+1\n",
    "    \n",
    "# # new_df = new_df.reset_index(drop=True)\n",
    "# # print(new_df)\n",
    "\n",
    "# i=0\n",
    "# for x in X_trainv:\n",
    "#     X_trainv = np.vstack((X_trainv,X_trainv[i]))\n",
    "#     y_trainv = np.vstack((y_trainv,y_trainv[i]))    \n",
    "#     atr_trainv = np.hstack((atr_trainv,atr_trainv[i]))\n",
    "#     i=i+1\n",
    "    \n",
    "# print(X_trainv)\n",
    "# print(len(X_trainv))\n",
    "# print(atr_trainv)\n",
    "# print(len(atr_trainv))\n",
    "\n",
    "# X_train = pd.DataFrame(X_trainv,columns = ['Num_of_nans', 'scaled_max_val', 'scaled_mean', 'scaled_min_val', 'num_of_dist_val','scaled_std_dev','castability','extractability', 'len_val'])\n",
    "# y_train = pd.DataFrame(y_trainv, columns =['y_act'])\n",
    "# print(X_train)\n",
    "# print(y_train)\n",
    "# # atr_train = pd.DataFrame(atr_trainv)\n",
    "# # print(atr_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927\n"
     ]
    }
   ],
   "source": [
    "arr = data['Attribute_name'].values\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2),analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "# print(X.toarray())\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "tempdf1 = pd.DataFrame(X1.toarray())\n",
    "tempdf2 = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1,tempdf,tempdf1,tempdf2], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n",
      "7400\n",
      "5122\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(data2,y, test_size=0.2,random_state=100)\n",
    "\n",
    "key_name = data['Attribute_name']\n",
    "atr_train,atr_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "# atr_train.reset_index(inplace=True,drop=True)\n",
    "# atr_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "print(len(X_train))\n",
    "# print(atr_train)\n",
    "list_sentences_train = atr_train.values\n",
    "# list_sentences_train = atr_trainv\n",
    "list_sentences_test = atr_test.values\n",
    "# print(list_sentences_train)\n",
    "\n",
    "structured_data_train = X_train \n",
    "structured_data_test = X_test\n",
    "\n",
    "print(len(structured_data_train))\n",
    "print(len(list(structured_data_train.loc[2])))\n",
    "\n",
    "structured_input_train = []\n",
    "for i in range(len(structured_data_train)):\n",
    "    if i%100 == 0:\n",
    "        print(i)\n",
    "    structured_input_train.append(list(structured_data_train.loc[i]))\n",
    "structured_input_train = np.array(structured_input_train).reshape(len(structured_data_train),len(structured_data_train.keys()))    \n",
    "\n",
    "structured_input_test = []\n",
    "for i in range(len(structured_data_train)):\n",
    "    structured_input_test.append(list(structured_data_train.loc[i]))\n",
    "structured_input_test = np.array(structured_input_test).reshape(len(structured_data_train),len(structured_data_train.keys()))    \n",
    "\n",
    "print(len(structured_data_train))\n",
    "print(len(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_name = data['sample_1']\n",
    "samp1_train,samp1_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "samp1_train.reset_index(inplace=True,drop=True)\n",
    "samp1_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "key_name = data['sample_2']\n",
    "samp2_train,samp2_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "samp2_train.reset_index(inplace=True,drop=True)\n",
    "samp2_test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n",
      "['#NULL!' '0' '268' ... 'Hate' '-0.101' '2']\n",
      "['0' '78' '110' ... nan '0.171' '5']\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_sentences_train = atr_train.values\n",
    "list_sentences_test = atr_test.values\n",
    "\n",
    "print(len(list_sentences_train))\n",
    "\n",
    "# X_train.sample_1 = X_train.sample_1.astype(str)\n",
    "# X_test.sample_1 = X_test.sample_1.astype(str)\n",
    "\n",
    "list_sentences_train1 = samp1_train.values\n",
    "list_sentences_test1 = samp1_test.values\n",
    "\n",
    "print(list_sentences_train1)\n",
    "\n",
    "# X_train.sample_2 = X_train.sample_2.astype(str)\n",
    "# X_test.sample_2 = X_test.sample_2.astype(str)\n",
    "\n",
    "list_sentences_train2 = samp2_train.values\n",
    "list_sentences_test2 = samp2_test.values\n",
    "\n",
    "print(list_sentences_train2)\n",
    "\n",
    "for i in range(len(list_sentences_train)):\n",
    "    list_sentences_train[i] = str(list_sentences_train[i])\n",
    "    list_sentences_train1[i] = str(list_sentences_train1[i])\n",
    "    list_sentences_train2[i] = str(list_sentences_train2[i])\n",
    "    \n",
    "for i in range(len(list_sentences_test)):\n",
    "    list_sentences_test[i] = str(list_sentences_test[i])\n",
    "    list_sentences_test1[i] = str(list_sentences_test1[i])\n",
    "    list_sentences_test2[i] = str(list_sentences_test2[i])    \n",
    "\n",
    "print(len(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "tokenizer1 = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train1))\n",
    "# train data\n",
    "list_tokenized_train1 = tokenizer.texts_to_sequences(list_sentences_train1)\n",
    "X_t1 = keras_seq.pad_sequences(list_tokenized_train1, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test1 = tokenizer.texts_to_sequences(list_sentences_test1)\n",
    "X_te1 = keras_seq.pad_sequences(list_tokenized_test1, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# tokenizer2 = keras_text.Tokenizer(char_level = True)\n",
    "# tokenizer2.fit_on_texts(list(list_sentences_train1))\n",
    "# # train data\n",
    "# list_tokenized_train2 = tokenizer.texts_to_sequences(list_sentences_train2)\n",
    "# X_t2 = keras_seq.pad_sequences(list_tokenized_train2, maxlen=maxlen)\n",
    "# # test data\n",
    "# list_tokenized_test2 = tokenizer.texts_to_sequences(list_sentences_test2)\n",
    "# X_te2 = keras_seq.pad_sequences(list_tokenized_test2, maxlen=maxlen)\n",
    "\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(numfilters,embed_size):\n",
    "    inp = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer.word_counts)+1, output_dim = embed_size)(inp)\n",
    "    prefilt_x = Dropout(0.5)(x)\n",
    "    out_conv = []\n",
    "\n",
    "    x = prefilt_x\n",
    "    for i in range(2):\n",
    "        x = Conv1D(numfilters, kernel_size = 3, activation = 'tanh')(x)\n",
    "        numfilters = numfilters*2\n",
    "    \n",
    "    out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    out_conv += [GlobalMaxPool1D()(x)]\n",
    "    xy = concatenate(out_conv, axis = -1)  \n",
    "    print(xy.shape)\n",
    "    #########################################################\n",
    "    inp1 = Input(shape=(None, ))\n",
    "    x = Embedding(input_dim = len(tokenizer1.word_counts)+1, output_dim = embed_size)(inp1)\n",
    "    prefilt_x = Dropout(0.25)(x)\n",
    "    out_conv = []\n",
    "\n",
    "    x = prefilt_x\n",
    "    for i in range(2):\n",
    "        x = Conv1D(16*2**(i), kernel_size = 3, activation = 'relu')(x)\n",
    "    out_conv += [Dropout(0.5)(GlobalMaxPool1D()(x))]\n",
    "    out_conv += [GlobalMaxPool1D()(x)]\n",
    "    x1 = concatenate(out_conv, axis = -1)\n",
    "    print(x1.shape)\n",
    "    \n",
    "    Str_input = Input(shape=(3195,))\n",
    "    layersfin = keras.layers.concatenate([xy,Str_input])\n",
    "    print(layersfin.shape)\n",
    "    x = Dense(500, activation='relu')(layersfin)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dropout(0.75)(x)\n",
    "    x = Dense(5, activation='softmax')(x)\n",
    "    model = Model(inputs=[inp,Str_input], outputs=[x])\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# model = build_model()\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32 # large enough that some other labels come in\n",
    "epochs = 25\n",
    "\n",
    "file_path=\"best_weights.h5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "# history = model.fit([X_t,X_t1,structured_data_train], to_categorical(y_train),\n",
    "#                     validation_data=([X_te,X_te1,structured_data_test], to_categorical(y_test)),\n",
    "#                     batch_size=batch_size, epochs=epochs, shuffle = True, callbacks=callbacks_list)\n",
    "\n",
    "# # model = build_model()\n",
    "# # model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  0 ...  2 11  2]\n",
      " [ 0  0  0 ... 37 22 39]\n",
      " [ 0  0  0 ... 15  5 23]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(7400, 512)\n",
      "7400\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9ec1765ce167>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# print(y_train[1851:])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mstructured_data_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructured_data_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "print(X_t)\n",
    "print(X_t.shape)\n",
    "# print(y_train[1851:])\n",
    "print(len(y_train))\n",
    "y_train = y_train.values\n",
    "structured_data_train = structured_data_train.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[[ 0  0  0 ...  3 11 10]\n",
      " [ 0  0  0 ... 22  6 33]\n",
      " [ 0  0  0 ...  4  1  2]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(4440, 1)\n",
      "(1480, 1)\n",
      "(4440, 3195)\n",
      "(1480, 3195)\n",
      "(?, 40)\n",
      "(?, 64)\n",
      "(?, 3235)\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 3323)\n",
      "Train on 5920 samples, validate on 1480 samples\n",
      "Epoch 1/25\n",
      "5920/5920 [==============================] - 3s 530us/step - loss: 1.1139 - acc: 0.5789 - val_loss: 0.6752 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.76892, saving model to best_weights0.h5\n",
      "Epoch 2/25\n",
      "5920/5920 [==============================] - 1s 232us/step - loss: 0.7043 - acc: 0.7517 - val_loss: 0.5037 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.76892 to 0.82432, saving model to best_weights0.h5\n",
      "Epoch 3/25\n",
      "5920/5920 [==============================] - 1s 232us/step - loss: 0.5555 - acc: 0.8096 - val_loss: 0.4607 - val_acc: 0.8426\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.82432 to 0.84257, saving model to best_weights0.h5\n",
      "Epoch 4/25\n",
      "5920/5920 [==============================] - 1s 231us/step - loss: 0.4566 - acc: 0.8417 - val_loss: 0.4365 - val_acc: 0.8507\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.84257 to 0.85068, saving model to best_weights0.h5\n",
      "Epoch 5/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.4024 - acc: 0.8721 - val_loss: 0.4362 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.85068 to 0.85203, saving model to best_weights0.h5\n",
      "Epoch 6/25\n",
      "5920/5920 [==============================] - 1s 232us/step - loss: 0.3691 - acc: 0.8812 - val_loss: 0.4279 - val_acc: 0.8541\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85203 to 0.85405, saving model to best_weights0.h5\n",
      "Epoch 7/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.3167 - acc: 0.8936 - val_loss: 0.4073 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.85405 to 0.87500, saving model to best_weights0.h5\n",
      "Epoch 8/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.2945 - acc: 0.9037 - val_loss: 0.3959 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.87500 to 0.87635, saving model to best_weights0.h5\n",
      "Epoch 9/25\n",
      "5920/5920 [==============================] - 1s 231us/step - loss: 0.2740 - acc: 0.9105 - val_loss: 0.4295 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.87635\n",
      "Epoch 10/25\n",
      "5920/5920 [==============================] - 1s 231us/step - loss: 0.2312 - acc: 0.9223 - val_loss: 0.4363 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.87635\n",
      "Epoch 11/25\n",
      "5920/5920 [==============================] - 1s 232us/step - loss: 0.2288 - acc: 0.9233 - val_loss: 0.4498 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.87635\n",
      "Epoch 12/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.2269 - acc: 0.9304 - val_loss: 0.4542 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.87635\n",
      "Epoch 13/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.2108 - acc: 0.9345 - val_loss: 0.4673 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.87635\n",
      "Epoch 14/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.1978 - acc: 0.9372 - val_loss: 0.4739 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.87635 to 0.87770, saving model to best_weights0.h5\n",
      "Epoch 15/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.1894 - acc: 0.9385 - val_loss: 0.4866 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87770\n",
      "Epoch 16/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.1960 - acc: 0.9348 - val_loss: 0.4702 - val_acc: 0.8818\n",
      "\n",
      "Epoch 00016: val_acc improved from 0.87770 to 0.88176, saving model to best_weights0.h5\n",
      "Epoch 17/25\n",
      "5920/5920 [==============================] - 1s 232us/step - loss: 0.1745 - acc: 0.9448 - val_loss: 0.4842 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88176\n",
      "Epoch 18/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.1714 - acc: 0.9486 - val_loss: 0.5167 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88176\n",
      "Epoch 19/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.1630 - acc: 0.9463 - val_loss: 0.5247 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88176\n",
      "Epoch 20/25\n",
      "5920/5920 [==============================] - 1s 231us/step - loss: 0.1640 - acc: 0.9524 - val_loss: 0.5351 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88176\n",
      "Epoch 21/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.1645 - acc: 0.9524 - val_loss: 0.5367 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88176\n",
      "Epoch 22/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.1554 - acc: 0.9554 - val_loss: 0.5337 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88176\n",
      "Epoch 23/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.1418 - acc: 0.9544 - val_loss: 0.5316 - val_acc: 0.8804\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88176\n",
      "Epoch 24/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.1378 - acc: 0.9581 - val_loss: 0.5828 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88176\n",
      "Epoch 25/25\n",
      "5920/5920 [==============================] - 1s 232us/step - loss: 0.1434 - acc: 0.9595 - val_loss: 0.5645 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.88176\n",
      "1480/1480 [==============================] - 0s 47us/step\n",
      "[0.5645007672342094, 0.8783783783783784]\n",
      "5920/5920 [==============================] - 0s 47us/step\n",
      "(0.04626801402238827, 0.9841216216216216)\n",
      "1480/1480 [==============================] - 0s 48us/step\n",
      "(0.5645007672342094, 0.8783783783783784)\n",
      "1851/1851 [==============================] - 0s 65us/step\n",
      "(0.5777042993479713, 0.8854673150614878)\n",
      "The training accuracy is:\n",
      "0.9841216216216216\n",
      "The validation accuracy is:\n",
      "0.8783783783783784\n",
      "The test accuracy is:\n",
      "0.8854673150614878\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 0  0  0 ...  2 11  2]\n",
      " [ 0  0  0 ... 37 22 39]\n",
      " [ 0  0  0 ... 15  5 23]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(4440, 1)\n",
      "(1480, 1)\n",
      "(4440, 3195)\n",
      "(1480, 3195)\n",
      "(?, 40)\n",
      "(?, 64)\n",
      "(?, 3235)\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 3323)\n",
      "Train on 5920 samples, validate on 1480 samples\n",
      "Epoch 1/25\n",
      "5920/5920 [==============================] - 3s 528us/step - loss: 1.1070 - acc: 0.5858 - val_loss: 0.7029 - val_acc: 0.7588\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75878, saving model to best_weights1.h5\n",
      "Epoch 2/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.6827 - acc: 0.7730 - val_loss: 0.5677 - val_acc: 0.8068\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75878 to 0.80676, saving model to best_weights1.h5\n",
      "Epoch 3/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.5452 - acc: 0.8098 - val_loss: 0.5185 - val_acc: 0.8304\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80676 to 0.83041, saving model to best_weights1.h5\n",
      "Epoch 4/25\n",
      "5920/5920 [==============================] - 1s 234us/step - loss: 0.4574 - acc: 0.8441 - val_loss: 0.4799 - val_acc: 0.8466\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.83041 to 0.84662, saving model to best_weights1.h5\n",
      "Epoch 5/25\n",
      "5920/5920 [==============================] - 1s 248us/step - loss: 0.3893 - acc: 0.8667 - val_loss: 0.4606 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.84662 to 0.85270, saving model to best_weights1.h5\n",
      "Epoch 6/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.3352 - acc: 0.8899 - val_loss: 0.4462 - val_acc: 0.8561\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85270 to 0.85608, saving model to best_weights1.h5\n",
      "Epoch 7/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.3021 - acc: 0.8986 - val_loss: 0.4700 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.85608 to 0.86216, saving model to best_weights1.h5\n",
      "Epoch 8/25\n",
      "5920/5920 [==============================] - 1s 231us/step - loss: 0.2892 - acc: 0.9034 - val_loss: 0.4970 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.86216 to 0.86486, saving model to best_weights1.h5\n",
      "Epoch 9/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.2584 - acc: 0.9132 - val_loss: 0.4961 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86486\n",
      "Epoch 10/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.2537 - acc: 0.9236 - val_loss: 0.5223 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86486\n",
      "Epoch 11/25\n",
      "5920/5920 [==============================] - 1s 248us/step - loss: 0.2278 - acc: 0.9243 - val_loss: 0.5015 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86486\n",
      "Epoch 12/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.2151 - acc: 0.9314 - val_loss: 0.5437 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.86486 to 0.86824, saving model to best_weights1.h5\n",
      "Epoch 13/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.2014 - acc: 0.9378 - val_loss: 0.5501 - val_acc: 0.8649\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86824\n",
      "Epoch 14/25\n",
      "5920/5920 [==============================] - 1s 234us/step - loss: 0.2000 - acc: 0.9365 - val_loss: 0.5685 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.86824\n",
      "Epoch 15/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.1955 - acc: 0.9407 - val_loss: 0.5784 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.86824\n",
      "Epoch 16/25\n",
      "5920/5920 [==============================] - 1s 239us/step - loss: 0.1744 - acc: 0.9437 - val_loss: 0.6068 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.86824\n",
      "Epoch 17/25\n",
      "5920/5920 [==============================] - 1s 234us/step - loss: 0.1640 - acc: 0.9483 - val_loss: 0.5938 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.86824\n",
      "Epoch 18/25\n",
      "5920/5920 [==============================] - 1s 234us/step - loss: 0.1768 - acc: 0.9468 - val_loss: 0.5979 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.86824\n",
      "Epoch 19/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.1686 - acc: 0.9492 - val_loss: 0.6186 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.86824\n",
      "Epoch 20/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.1575 - acc: 0.9492 - val_loss: 0.6031 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00020: val_acc improved from 0.86824 to 0.87027, saving model to best_weights1.h5\n",
      "Epoch 21/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.1489 - acc: 0.9537 - val_loss: 0.6054 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87027\n",
      "Epoch 22/25\n",
      "5920/5920 [==============================] - 1s 233us/step - loss: 0.1605 - acc: 0.9541 - val_loss: 0.6055 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87027\n",
      "Epoch 23/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.1462 - acc: 0.9527 - val_loss: 0.6475 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87027\n",
      "Epoch 24/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.1371 - acc: 0.9551 - val_loss: 0.6701 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00024: val_acc improved from 0.87027 to 0.87162, saving model to best_weights1.h5\n",
      "Epoch 25/25\n",
      "5920/5920 [==============================] - 1s 234us/step - loss: 0.1400 - acc: 0.9617 - val_loss: 0.6865 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87162\n",
      "1480/1480 [==============================] - 0s 49us/step\n",
      "[0.6864843579562935, 0.8655405405405405]\n",
      "5920/5920 [==============================] - 0s 48us/step\n",
      "(0.041158977484745854, 0.9871621621621621)\n",
      "1480/1480 [==============================] - 0s 49us/step\n",
      "(0.6864843579562935, 0.8655405405405405)\n",
      "1851/1851 [==============================] - 0s 71us/step\n",
      "(0.5790173699893157, 0.8800648299183219)\n",
      "The training accuracy is:\n",
      "0.9871621621621621\n",
      "The validation accuracy is:\n",
      "0.8655405405405405\n",
      "The test accuracy is:\n",
      "0.8800648299183219\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 0  0  0 ...  2 11  2]\n",
      " [ 0  0  0 ... 37 22 39]\n",
      " [ 0  0  0 ... 15  5 23]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(4440, 1)\n",
      "(1480, 1)\n",
      "(4440, 3195)\n",
      "(1480, 3195)\n",
      "(?, 40)\n",
      "(?, 64)\n",
      "(?, 3235)\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 3323)\n",
      "Train on 5920 samples, validate on 1480 samples\n",
      "Epoch 1/25\n",
      "5920/5920 [==============================] - 3s 591us/step - loss: 1.1296 - acc: 0.5828 - val_loss: 0.6836 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.75203, saving model to best_weights2.h5\n",
      "Epoch 2/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.7232 - acc: 0.7471 - val_loss: 0.5094 - val_acc: 0.8034\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.75203 to 0.80338, saving model to best_weights2.h5\n",
      "Epoch 3/25\n",
      "5920/5920 [==============================] - 1s 252us/step - loss: 0.5611 - acc: 0.8110 - val_loss: 0.4794 - val_acc: 0.8270\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.80338 to 0.82703, saving model to best_weights2.h5\n",
      "Epoch 4/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.4629 - acc: 0.8459 - val_loss: 0.4146 - val_acc: 0.8446\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.82703 to 0.84459, saving model to best_weights2.h5\n",
      "Epoch 5/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.4014 - acc: 0.8682 - val_loss: 0.4147 - val_acc: 0.8554\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.84459 to 0.85541, saving model to best_weights2.h5\n",
      "Epoch 6/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.3620 - acc: 0.8804 - val_loss: 0.4152 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85541\n",
      "Epoch 7/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.3234 - acc: 0.8922 - val_loss: 0.4264 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.85541 to 0.86689, saving model to best_weights2.h5\n",
      "Epoch 8/25\n",
      "5920/5920 [==============================] - 1s 239us/step - loss: 0.3118 - acc: 0.9051 - val_loss: 0.4293 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86689\n",
      "Epoch 9/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.2915 - acc: 0.9105 - val_loss: 0.4591 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86689\n",
      "Epoch 10/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.2563 - acc: 0.9204 - val_loss: 0.4461 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86689\n",
      "Epoch 11/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.2381 - acc: 0.9235 - val_loss: 0.4486 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00011: val_acc did not improve from 0.86689\n",
      "Epoch 12/25\n",
      "5920/5920 [==============================] - 1s 239us/step - loss: 0.2243 - acc: 0.9275 - val_loss: 0.4670 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86689\n",
      "Epoch 13/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.2120 - acc: 0.9351 - val_loss: 0.4621 - val_acc: 0.8777\n",
      "\n",
      "Epoch 00013: val_acc improved from 0.86689 to 0.87770, saving model to best_weights2.h5\n",
      "Epoch 14/25\n",
      "5920/5920 [==============================] - 1s 234us/step - loss: 0.1977 - acc: 0.9345 - val_loss: 0.4662 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.87770\n",
      "Epoch 15/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.2075 - acc: 0.9356 - val_loss: 0.4839 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87770\n",
      "Epoch 16/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.1940 - acc: 0.9348 - val_loss: 0.4897 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87770\n",
      "Epoch 17/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.1797 - acc: 0.9443 - val_loss: 0.5057 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87770\n",
      "Epoch 18/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.1698 - acc: 0.9503 - val_loss: 0.5013 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00018: val_acc improved from 0.87770 to 0.87838, saving model to best_weights2.h5\n",
      "Epoch 19/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.1699 - acc: 0.9475 - val_loss: 0.5080 - val_acc: 0.8770\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87838\n",
      "Epoch 20/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.1569 - acc: 0.9517 - val_loss: 0.5191 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87838\n",
      "Epoch 21/25\n",
      "5920/5920 [==============================] - 1s 250us/step - loss: 0.1687 - acc: 0.9497 - val_loss: 0.5397 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87838\n",
      "Epoch 22/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.1627 - acc: 0.9510 - val_loss: 0.5724 - val_acc: 0.8615\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87838\n",
      "Epoch 23/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.1542 - acc: 0.9527 - val_loss: 0.5408 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87838\n",
      "Epoch 24/25\n",
      "5920/5920 [==============================] - 1s 239us/step - loss: 0.1522 - acc: 0.9541 - val_loss: 0.5568 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87838\n",
      "Epoch 25/25\n",
      "5920/5920 [==============================] - 1s 247us/step - loss: 0.1442 - acc: 0.9556 - val_loss: 0.5773 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87838\n",
      "1480/1480 [==============================] - 0s 61us/step\n",
      "[0.5773374374855209, 0.8635135135135135]\n",
      "5920/5920 [==============================] - 0s 51us/step\n",
      "(0.04895796819048858, 0.9841216216216216)\n",
      "1480/1480 [==============================] - 0s 49us/step\n",
      "(0.5773374374855209, 0.8635135135135135)\n",
      "1851/1851 [==============================] - 0s 72us/step\n",
      "(0.5768033270897703, 0.8800648302403351)\n",
      "The training accuracy is:\n",
      "0.9841216216216216\n",
      "The validation accuracy is:\n",
      "0.8635135135135135\n",
      "The test accuracy is:\n",
      "0.8800648302403351\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 0  0  0 ...  2 11  2]\n",
      " [ 0  0  0 ... 37 22 39]\n",
      " [ 0  0  0 ... 15  5 23]\n",
      " ...\n",
      " [ 0  0  0 ... 18  5  8]\n",
      " [ 0  0  0 ... 28 16 33]\n",
      " [ 0  0  0 ...  4  1  2]]\n",
      "(4440, 1)\n",
      "(1480, 1)\n",
      "(4440, 3195)\n",
      "(1480, 3195)\n",
      "(?, 40)\n",
      "(?, 64)\n",
      "(?, 3235)\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 3323)\n",
      "Train on 5920 samples, validate on 1480 samples\n",
      "Epoch 1/25\n",
      "5920/5920 [==============================] - 4s 605us/step - loss: 1.0774 - acc: 0.5934 - val_loss: 0.7215 - val_acc: 0.7486\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.74865, saving model to best_weights3.h5\n",
      "Epoch 2/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.7112 - acc: 0.7500 - val_loss: 0.5287 - val_acc: 0.8243\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.74865 to 0.82432, saving model to best_weights3.h5\n",
      "Epoch 3/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.5342 - acc: 0.8135 - val_loss: 0.4670 - val_acc: 0.8500\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.82432 to 0.85000, saving model to best_weights3.h5\n",
      "Epoch 4/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.4396 - acc: 0.8443 - val_loss: 0.4521 - val_acc: 0.8581\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.85000 to 0.85811, saving model to best_weights3.h5\n",
      "Epoch 5/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.3887 - acc: 0.8647 - val_loss: 0.4526 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85811\n",
      "Epoch 6/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.3486 - acc: 0.8838 - val_loss: 0.4543 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.85811 to 0.86351, saving model to best_weights3.h5\n",
      "Epoch 7/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.3330 - acc: 0.8907 - val_loss: 0.4619 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.86351 to 0.86959, saving model to best_weights3.h5\n",
      "Epoch 8/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.2929 - acc: 0.9047 - val_loss: 0.4623 - val_acc: 0.8655\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.86959\n",
      "Epoch 9/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.2807 - acc: 0.9122 - val_loss: 0.4488 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.86959\n",
      "Epoch 10/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.2476 - acc: 0.9198 - val_loss: 0.4758 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00010: val_acc improved from 0.86959 to 0.87095, saving model to best_weights3.h5\n",
      "Epoch 11/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.2426 - acc: 0.9262 - val_loss: 0.4613 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.87095 to 0.87500, saving model to best_weights3.h5\n",
      "Epoch 12/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.2112 - acc: 0.9301 - val_loss: 0.4792 - val_acc: 0.8811\n",
      "\n",
      "Epoch 00012: val_acc improved from 0.87500 to 0.88108, saving model to best_weights3.h5\n",
      "Epoch 13/25\n",
      "5920/5920 [==============================] - 1s 239us/step - loss: 0.2089 - acc: 0.9333 - val_loss: 0.4866 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.88108\n",
      "Epoch 14/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.2113 - acc: 0.9338 - val_loss: 0.4850 - val_acc: 0.8730\n",
      "\n",
      "Epoch 00014: val_acc did not improve from 0.88108\n",
      "Epoch 15/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.1973 - acc: 0.9377 - val_loss: 0.5003 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.88108\n",
      "Epoch 16/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.1709 - acc: 0.9481 - val_loss: 0.4998 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.88108\n",
      "Epoch 17/25\n",
      "5920/5920 [==============================] - 1s 238us/step - loss: 0.1764 - acc: 0.9421 - val_loss: 0.5009 - val_acc: 0.8784\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.88108\n",
      "Epoch 18/25\n",
      "5920/5920 [==============================] - 1s 236us/step - loss: 0.1814 - acc: 0.9443 - val_loss: 0.5230 - val_acc: 0.8757\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.88108\n",
      "Epoch 19/25\n",
      "5920/5920 [==============================] - 1s 235us/step - loss: 0.1650 - acc: 0.9458 - val_loss: 0.5493 - val_acc: 0.8764\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.88108\n",
      "Epoch 20/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.1656 - acc: 0.9507 - val_loss: 0.5449 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.88108\n",
      "Epoch 21/25\n",
      "5920/5920 [==============================] - 1s 247us/step - loss: 0.1542 - acc: 0.9535 - val_loss: 0.5320 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.88108\n",
      "Epoch 22/25\n",
      "5920/5920 [==============================] - 1s 243us/step - loss: 0.1549 - acc: 0.9517 - val_loss: 0.5396 - val_acc: 0.8703\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.88108\n",
      "Epoch 23/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.1457 - acc: 0.9544 - val_loss: 0.5719 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.88108\n",
      "Epoch 24/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.1458 - acc: 0.9530 - val_loss: 0.5235 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.88108\n",
      "Epoch 25/25\n",
      "5920/5920 [==============================] - 1s 237us/step - loss: 0.1446 - acc: 0.9573 - val_loss: 0.5524 - val_acc: 0.8845\n",
      "\n",
      "Epoch 00025: val_acc improved from 0.88108 to 0.88446, saving model to best_weights3.h5\n",
      "1480/1480 [==============================] - 0s 52us/step\n",
      "[0.5523763904700408, 0.8844594594594595]\n",
      "5920/5920 [==============================] - 0s 51us/step\n",
      "(0.0467542648982458, 0.9863175675675676)\n",
      "1480/1480 [==============================] - 0s 51us/step\n",
      "(0.5523763904700408, 0.8844594594594595)\n",
      "1851/1851 [==============================] - 0s 71us/step\n",
      "(0.5694505965336151, 0.8795245811785961)\n",
      "The training accuracy is:\n",
      "0.9863175675675676\n",
      "The validation accuracy is:\n",
      "0.8844594594594595\n",
      "The test accuracy is:\n",
      "0.8795245811785961\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "[[ 0  0  0 ...  2 11  2]\n",
      " [ 0  0  0 ... 37 22 39]\n",
      " [ 0  0  0 ... 15  5 23]\n",
      " ...\n",
      " [ 0  0  0 ...  9  3  7]\n",
      " [ 0  0  0 ... 11  1 24]\n",
      " [ 0  0  0 ... 19  1  4]]\n",
      "(4440, 1)\n",
      "(1480, 1)\n",
      "(4440, 3195)\n",
      "(1480, 3195)\n",
      "(?, 40)\n",
      "(?, 64)\n",
      "(?, 3235)\n",
      "(?, 128)\n",
      "(?, 64)\n",
      "(?, 3323)\n",
      "Train on 5920 samples, validate on 1480 samples\n",
      "Epoch 1/25\n",
      "5920/5920 [==============================] - 4s 630us/step - loss: 1.0987 - acc: 0.5941 - val_loss: 0.7658 - val_acc: 0.7372\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.73716, saving model to best_weights4.h5\n",
      "Epoch 2/25\n",
      "5920/5920 [==============================] - 1s 244us/step - loss: 0.6890 - acc: 0.7637 - val_loss: 0.5619 - val_acc: 0.8149\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.73716 to 0.81486, saving model to best_weights4.h5\n",
      "Epoch 3/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.5441 - acc: 0.8130 - val_loss: 0.5033 - val_acc: 0.8345\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.81486 to 0.83446, saving model to best_weights4.h5\n",
      "Epoch 4/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.4556 - acc: 0.8427 - val_loss: 0.4626 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.83446 to 0.85473, saving model to best_weights4.h5\n",
      "Epoch 5/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.3798 - acc: 0.8709 - val_loss: 0.4645 - val_acc: 0.8453\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.85473\n",
      "Epoch 6/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.3384 - acc: 0.8904 - val_loss: 0.4619 - val_acc: 0.8547\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.85473\n",
      "Epoch 7/25\n",
      "5920/5920 [==============================] - 1s 251us/step - loss: 0.3072 - acc: 0.8953 - val_loss: 0.4935 - val_acc: 0.8459\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.85473\n",
      "Epoch 8/25\n",
      "5920/5920 [==============================] - 1s 246us/step - loss: 0.2924 - acc: 0.9042 - val_loss: 0.4615 - val_acc: 0.8601\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.85473 to 0.86014, saving model to best_weights4.h5\n",
      "Epoch 9/25\n",
      "5920/5920 [==============================] - 1s 250us/step - loss: 0.2659 - acc: 0.9122 - val_loss: 0.4735 - val_acc: 0.8662\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.86014 to 0.86622, saving model to best_weights4.h5\n",
      "Epoch 10/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.2454 - acc: 0.9167 - val_loss: 0.4808 - val_acc: 0.8642\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.86622\n",
      "Epoch 11/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.2285 - acc: 0.9238 - val_loss: 0.4848 - val_acc: 0.8676\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.86622 to 0.86757, saving model to best_weights4.h5\n",
      "Epoch 12/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.2124 - acc: 0.9282 - val_loss: 0.5061 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.86757\n",
      "Epoch 13/25\n",
      "5920/5920 [==============================] - 1s 243us/step - loss: 0.2051 - acc: 0.9323 - val_loss: 0.5455 - val_acc: 0.8622\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.86757\n",
      "Epoch 14/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.2024 - acc: 0.9326 - val_loss: 0.4965 - val_acc: 0.8750\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.86757 to 0.87500, saving model to best_weights4.h5\n",
      "Epoch 15/25\n",
      "5920/5920 [==============================] - 1s 243us/step - loss: 0.1834 - acc: 0.9407 - val_loss: 0.5161 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.87500\n",
      "Epoch 16/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.1734 - acc: 0.9432 - val_loss: 0.5487 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.87500\n",
      "Epoch 17/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.1826 - acc: 0.9402 - val_loss: 0.5423 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00017: val_acc did not improve from 0.87500\n",
      "Epoch 18/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.1812 - acc: 0.9419 - val_loss: 0.5353 - val_acc: 0.8669\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.87500\n",
      "Epoch 19/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.1585 - acc: 0.9517 - val_loss: 0.5555 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.87500\n",
      "Epoch 20/25\n",
      "5920/5920 [==============================] - 1s 241us/step - loss: 0.1643 - acc: 0.9458 - val_loss: 0.5550 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.87500\n",
      "Epoch 21/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.1637 - acc: 0.9514 - val_loss: 0.5578 - val_acc: 0.8736\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.87500\n",
      "Epoch 22/25\n",
      "5920/5920 [==============================] - 1s 240us/step - loss: 0.1397 - acc: 0.9564 - val_loss: 0.5690 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.87500\n",
      "Epoch 23/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.1417 - acc: 0.9554 - val_loss: 0.5330 - val_acc: 0.8696\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.87500\n",
      "Epoch 24/25\n",
      "5920/5920 [==============================] - 1s 242us/step - loss: 0.1500 - acc: 0.9498 - val_loss: 0.6010 - val_acc: 0.8709\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.87500\n",
      "Epoch 25/25\n",
      "5920/5920 [==============================] - 1s 244us/step - loss: 0.1350 - acc: 0.9581 - val_loss: 0.5705 - val_acc: 0.8682\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.87500\n",
      "1480/1480 [==============================] - 0s 52us/step\n",
      "[0.5704907984951058, 0.8682432432432432]\n",
      "5920/5920 [==============================] - 0s 51us/step\n",
      "(0.03968103824921166, 0.9853040540540541)\n",
      "1480/1480 [==============================] - 0s 50us/step\n",
      "(0.5704907984951058, 0.8682432432432432)\n",
      "1851/1851 [==============================] - 0s 72us/step\n",
      "(0.5435333583465722, 0.8903295516903371)\n",
      "The training accuracy is:\n",
      "0.9853040540540541\n",
      "The validation accuracy is:\n",
      "0.8682432432432432\n",
      "The test accuracy is:\n",
      "0.8903295516903371\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n_filters_grid = [16,32,64,128]\n",
    "# embed_size = [32,64,128,512]\n",
    "\n",
    "batch_size = 32 # large enough that some other labels come in\n",
    "epochs = 25\n",
    "\n",
    "k = 5\n",
    "kf = KFold(n_splits=k)\n",
    "n_filters_grid = [32]\n",
    "embed_size = [256]\n",
    "models = []\n",
    "\n",
    "avgsc_lst,avgsc_val_lst,avgsc_train_lst = [],[],[]\n",
    "avgsc,avgsc_val,avgsc_train = 0,0,0\n",
    "i=0\n",
    "for train_index, test_index in kf.split(X_t):\n",
    "    file_path= 'best_weights'+str(i)+'.h5'\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    early = EarlyStopping(monitor=\"val_acc\", mode=\"max\", patience=20)\n",
    "\n",
    "    callbacks_list = [checkpoint, early] #early\n",
    "        \n",
    "    print('\\n')\n",
    "    X_train_cur, X_test_cur = X_t[train_index], X_t[test_index]\n",
    "    y_train_cur, y_test_cur = y_train[train_index], y_train[test_index]\n",
    "    structured_data_train_cur, structured_data_test_cur = structured_data_train[train_index],structured_data_train[test_index]\n",
    "    \n",
    "    X_train_train, X_val,y_train_train,y_val = train_test_split(X_train_cur,y_train_cur, test_size=0.25,random_state=100)\n",
    "    structured_data_train_train,structured_data_val = train_test_split(structured_data_train_cur, test_size=0.25,random_state=100)\n",
    "    \n",
    "    \n",
    "    print(X_train_cur)\n",
    "    \n",
    "    print(y_train_train.shape)\n",
    "    print(y_val.shape)\n",
    "    print(structured_data_train_train.shape)\n",
    "    print(structured_data_val.shape)\n",
    "    \n",
    "    bestPerformingModel = build_model(10,5)\n",
    "    bestscore = 0\n",
    "    for ne in n_filters_grid:\n",
    "        for md in embed_size:\n",
    "#             clf = RandomForestClassifier(n_estimators=ne,max_depth=md)\n",
    "            clf = build_model(ne,md)\n",
    "            history = clf.fit([X_train_cur,structured_data_train_cur], to_categorical(y_train_cur),\n",
    "                    validation_data=([X_test_cur,structured_data_test_cur], to_categorical(y_test_cur)),\n",
    "                    batch_size=batch_size, epochs=epochs, shuffle = True, callbacks=callbacks_list)\n",
    "        \n",
    "            sc = clf.evaluate([X_test_cur,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "            bestPerformingModel = clf\n",
    "            if bestscore < sc:\n",
    "                bestscore = sc\n",
    "                bestPerformingModel = clf\n",
    "\n",
    "#     clf.load_weights('best_weights'+str(i)+'.h5')\n",
    "    print(sc)\n",
    "    avgsc_val_lst.append(sc[1])\n",
    "    \n",
    "    loss, bscr_train = bestPerformingModel.evaluate([X_train_cur,structured_data_train_cur],to_categorical(y_train_cur))\n",
    "    print(loss, bscr_train)\n",
    "    loss, bscr_val = bestPerformingModel.evaluate([X_test_cur,structured_data_test_cur],to_categorical(y_test_cur))\n",
    "    print(loss, bscr_val)    \n",
    "    loss, bscr = bestPerformingModel.evaluate([X_te,structured_data_test],to_categorical(y_test))\n",
    "    print(loss, bscr)\n",
    "    \n",
    "    models.append(clf)\n",
    "    \n",
    "    avgsc = avgsc + bscr\n",
    "    \n",
    "    avgsc_train = avgsc_train + bscr_train\n",
    "    avgsc_val = avgsc_val + bscr_val\n",
    "    avgsc_lst.append(bscr)\n",
    "    avgsc_train_lst.append(bscr_train)\n",
    "    print('The training accuracy is:')\n",
    "    print(bscr_train)\n",
    "    print('The validation accuracy is:')\n",
    "    print(bscr_val)    \n",
    "    print('The test accuracy is:')    \n",
    "    print(bscr)\n",
    "    print('\\n')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<keras.engine.training.Model object at 0x7f844c19bad0>, <keras.engine.training.Model object at 0x7f81940d4710>, <keras.engine.training.Model object at 0x7f8b2e470dd0>, <keras.engine.training.Model object at 0x7f8b2d9088d0>, <keras.engine.training.Model object at 0x7f8b2cc00710>]\n",
      "[0.9841216216216216, 0.9871621621621621, 0.9841216216216216, 0.9863175675675676, 0.9853040540540541]\n",
      "[0.8783783783783784, 0.8655405405405405, 0.8635135135135135, 0.8844594594594595, 0.8682432432432432]\n",
      "[0.8854673150614878, 0.8800648299183219, 0.8800648302403351, 0.8795245811785961, 0.8903295516903371]\n",
      "0.9854054054054056\n",
      "0.872027027027027\n",
      "0.8830902216178156\n",
      "1851/1851 [==============================] - 0s 76us/step\n",
      "[[1.0000000e+00 5.8397471e-13 4.9001246e-14 4.3336051e-12 1.4921083e-08]\n",
      " [1.0000000e+00 1.7303493e-10 1.3577841e-09 5.4521276e-09 2.3144874e-10]\n",
      " [5.5389351e-06 1.1416386e-05 1.6488797e-04 1.1002002e-04 9.9970812e-01]\n",
      " ...\n",
      " [1.8275927e-05 8.9471841e-06 9.9628335e-01 3.4965996e-03 1.9293479e-04]\n",
      " [9.9442923e-01 8.7300077e-06 3.8278926e-05 1.5640560e-05 5.5081402e-03]\n",
      " [9.7039467e-01 8.1972405e-04 7.7476958e-03 4.9812323e-03 1.6056621e-02]]\n",
      "[[680   0  10   2  14]\n",
      " [  5 117  10   2   9]\n",
      " [ 18  13 386   7   7]\n",
      " [  5   2  20 131   9]\n",
      " [ 27   7  24  12 334]]\n"
     ]
    }
   ],
   "source": [
    "print(models)\n",
    "print(avgsc_train_lst)\n",
    "print(avgsc_val_lst)\n",
    "print(avgsc_lst)\n",
    "\n",
    "# i = avgsc_lst.index(max(avgsc_lst))\n",
    "# bestPerformingModel = models[i]\n",
    "\n",
    "print(avgsc_train/k)\n",
    "print(avgsc_val/k)\n",
    "print(avgsc/k)\n",
    "\n",
    "\n",
    "bestPerformingModel\n",
    "\n",
    "loss, bscr = bestPerformingModel.evaluate([X_te,structured_data_test],to_categorical(y_test))\n",
    "y_pred = bestPerformingModel.predict([X_te,structured_data_test])\n",
    "print(y_pred)\n",
    "y_pred = [np.argmax(i) for i in y_pred]\n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.38141545110750946, 0.07725553754727174, 0.23284710967044842, 0.0902215018908698, 0.2182603997839006)\n",
      "(706, 143, 431, 167, 404)\n"
     ]
    }
   ],
   "source": [
    "class0,class1,class2,class3,class4 = 0,0,0,0,0\n",
    "for x in y_test['y_act']:\n",
    "    if x==0:\n",
    "        class0 = class0+1\n",
    "    if x==1:\n",
    "        class1 = class1+1        \n",
    "    if x==2:\n",
    "        class2 = class2+1\n",
    "    if x==3:\n",
    "        class3 = class3+1\n",
    "    if x==4:\n",
    "        class4 = class4+1\n",
    "sm = class0+class1+class2+class3+class4\n",
    "print(class0*1.0/sm,class1*1.0/sm,class2*1.0/sm,class3*1.0/sm,class4*1.0/sm)\n",
    "print(class0,class1,class2,class3,class4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df = pd.DataFrame({'y_pred':y_pred})\n",
    "# print(y_pred)\n",
    "dfn = pd.concat([y_pred_df,y_test],axis=1)\n",
    "dfn.to_csv('dfny.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Usable directly numeric':0\n",
    "# 'Usable with extraction':1 \n",
    "# 'Usable directly categorical':2\n",
    "# 'Unusable':3\n",
    "# 'Context_specific':4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1851\n"
     ]
    }
   ],
   "source": [
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.read_csv('dfn.csv')\n",
    "newdf = newdf[newdf['y_act']!=newdf['y_pred']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Unnamed: 0                  Attribute_name  Num_of_nans  scaled_max_val  \\\n",
      "19            19                             SSR         99.0       -0.466200   \n",
      "40            40                       Reference          7.0        2.285627   \n",
      "60            60                           CHROM          0.0       -0.470610   \n",
      "61            61                       father_pk          1.0        2.285627   \n",
      "65            65                     toss_winner          0.0       -0.470610   \n",
      "70            70                             ibu         41.0       -0.432574   \n",
      "76            76                      s3p1c6lost         81.0       -0.443048   \n",
      "78            78                  Variable Label          0.0       -0.470610   \n",
      "87            87                         Version          0.0       -0.470610   \n",
      "106          106                   Facility Name         14.0       -0.470610   \n",
      "119          119                   n.valid_until          0.0       -0.470610   \n",
      "124          124                           CLNDN          0.0       -0.470610   \n",
      "126          126                           DATUM          0.0       -0.470610   \n",
      "131          131                       sub_group          0.0       -0.470610   \n",
      "148          148                       sub_group          0.0       -0.470610   \n",
      "149          149  ri_employee_referral_prog_from         81.0       -0.470610   \n",
      "160          160                             pc5         52.0       -0.454624   \n",
      "167          167                           CLNVC          0.0       -0.470610   \n",
      "172          172                     s2p2c6plant         47.0       -0.470610   \n",
      "173          173                             ALT          0.0       -0.470610   \n",
      "188          188                       vendor_id          0.0       -0.470610   \n",
      "192          192            group_photo.photo_id          0.0        2.285627   \n",
      "198          198        fBodyBodyGyroMag.maxInds          0.0       -0.470575   \n",
      "227          227                      s1p2c1area         80.0       -0.443048   \n",
      "235          235                          Precip          0.0       -0.470610   \n",
      "236          236                          report          0.0       -0.470610   \n",
      "241          241                     Amino_acids         15.0       -0.470610   \n",
      "253          253                              id          0.0       -0.301377   \n",
      "259          259                     soloistName          3.0       -0.470610   \n",
      "262          262                     region_name          0.0        2.285627   \n",
      "...          ...                             ...          ...             ...   \n",
      "1494        1494              fBodyAcc.maxInds.Z          0.0       -0.470462   \n",
      "1507        1507                     interviewer         24.0       -0.470610   \n",
      "1514        1514                   Variable Name          0.0       -0.470610   \n",
      "1549        1549       ri_employer_web_post_from         63.0       -0.470610   \n",
      "1550        1550                        othrhrf3         72.0       -0.470059   \n",
      "1562        1562                             CPI          7.0       -0.407499   \n",
      "1587        1587                        category          0.0       -0.470610   \n",
      "1588        1588                            WBAN          0.0        2.285627   \n",
      "1590        1590                         Aridity          0.0        2.285627   \n",
      "1598        1598             pw_source_name_9089          0.0       -0.470610   \n",
      "1603        1603          ji_offered_to_sec_j_fw         59.0       -0.470610   \n",
      "1638        1638                            GAge          0.0       -0.469783   \n",
      "1643        1643                       build1val         25.0        2.285627   \n",
      "1653        1653                       married38         99.0       -0.470335   \n",
      "1658        1658                         WebName         53.0       -0.470610   \n",
      "1680        1680                         tenure1          8.0       -0.466200   \n",
      "1700        1700                        MPI.pctn          0.0       -0.443048   \n",
      "1703        1703                        DHSCLUST          0.0        0.898137   \n",
      "1710        1710                      s2p1c5area         97.0       -0.451317   \n",
      "1711        1711                      s2p1c1area         67.0       -0.443048   \n",
      "1753        1753                       rentplot3         83.0        2.285627   \n",
      "1769        1769           Main Source of Income          0.0       -0.470610   \n",
      "1776        1776                          numcol          0.0        2.285627   \n",
      "1784        1784                          StXbor          0.0       -0.470335   \n",
      "1791        1791                              No          0.0       -0.465649   \n",
      "1811        1811                        SurveyID         40.0       -0.470610   \n",
      "1816        1816                        subs2tot          3.0       -0.470610   \n",
      "1819        1819                    dropoff_site          0.0       -0.470610   \n",
      "1822        1822                         user_id          0.0       -0.470610   \n",
      "1827        1827                           Round         81.0       -0.470610   \n",
      "\n",
      "      scaled_mean  scaled_min_val  num_of_dist_val  scaled_std_dev  \\\n",
      "19      -0.320756        0.041688              0.0       -0.394172   \n",
      "40       2.934161        0.095459             88.0        2.753815   \n",
      "60      -0.321504        0.041257              0.0       -0.395500   \n",
      "61       2.897747        0.042118             30.0        1.707975   \n",
      "65      -0.321504        0.041257              2.0       -0.395500   \n",
      "70      -0.307598        0.042978              4.0       -0.387329   \n",
      "76      -0.321486        0.041257              0.0       -0.394758   \n",
      "78      -0.321504        0.041257             80.0       -0.395500   \n",
      "87      -0.321504        0.041257              0.0       -0.395500   \n",
      "106     -0.321504        0.041257              2.0       -0.395500   \n",
      "119     -0.321504        0.041257              0.0       -0.395500   \n",
      "124     -0.321504        0.041257             14.0       -0.395500   \n",
      "126     -0.321504        0.041257              0.0       -0.395500   \n",
      "131     -0.321504        0.041257              0.0       -0.395500   \n",
      "148     -0.321504        0.041257              0.0       -0.395500   \n",
      "149     -0.321504        0.041257              0.0       -0.395500   \n",
      "160     -0.316322        0.041257              0.0       -0.389143   \n",
      "167     -0.321504        0.041257              0.0       -0.395500   \n",
      "172     -0.321504        0.041257              0.0       -0.395500   \n",
      "173     -0.321504        0.041257              0.0       -0.395500   \n",
      "188     -0.321504        0.041257              0.0       -0.395500   \n",
      "192      2.934161        0.040827             59.0        2.753815   \n",
      "198     -0.321791        0.040827              1.0       -0.395451   \n",
      "227     -0.302747        0.041473              2.0       -0.385837   \n",
      "235     -0.321504        0.041257              0.0       -0.395500   \n",
      "236     -0.321504        0.041257              0.0       -0.395500   \n",
      "241     -0.321504        0.041257              1.0       -0.395500   \n",
      "253     -0.221392        0.041688            100.0       -0.339679   \n",
      "259     -0.321504        0.041257              0.0       -0.395500   \n",
      "262      2.934161        4.342932            100.0        2.753815   \n",
      "...           ...             ...              ...             ...   \n",
      "1494    -0.321775        0.040827              1.0       -0.395423   \n",
      "1507    -0.321504        0.041257              4.0       -0.395500   \n",
      "1514    -0.321504        0.041257             83.0       -0.395500   \n",
      "1549    -0.321504        0.041257              0.0       -0.395500   \n",
      "1550    -0.321503        0.041257              0.0       -0.395485   \n",
      "1562    -0.265356        0.095486             30.0       -0.382986   \n",
      "1587    -0.321504        0.041257              0.0       -0.395500   \n",
      "1588     2.934161        4.342932            100.0        2.753815   \n",
      "1590     2.934161       -4.259987             11.0        1.437161   \n",
      "1598    -0.321504        0.041257              0.0       -0.395500   \n",
      "1603    -0.321504        0.041257              0.0       -0.395500   \n",
      "1638    -0.321470        0.041257              0.0       -0.395350   \n",
      "1643     2.934161        0.041257              3.0        2.753815   \n",
      "1653    -0.321178        0.041688              0.0       -0.395500   \n",
      "1658    -0.321504        0.041257             40.0       -0.395500   \n",
      "1680    -0.320809        0.041257              0.0       -0.394915   \n",
      "1700    -0.317267        0.041257              2.0       -0.390725   \n",
      "1703    -0.122542        0.042548             10.0       -0.187187   \n",
      "1710    -0.316715        0.041283              0.0       -0.391257   \n",
      "1711    -0.305156        0.041257              2.0       -0.385313   \n",
      "1753     1.380286        0.041257              0.0        2.753815   \n",
      "1769    -0.321504        0.041257              0.0       -0.395500   \n",
      "1776     2.934161        0.901592             23.0        2.753815   \n",
      "1784    -0.321477        0.041257              0.0       -0.395413   \n",
      "1791    -0.318411        0.041688            100.0       -0.393866   \n",
      "1811    -0.321504        0.041257              0.0       -0.395500   \n",
      "1816    -0.321504        0.041257              1.0       -0.395500   \n",
      "1819    -0.321504        0.041257              0.0       -0.395500   \n",
      "1822    -0.321504        0.041257              0.0       -0.395500   \n",
      "1827    -0.321504        0.041257              0.0       -0.395500   \n",
      "\n",
      "      castability  extractability  len_val  y_pred  y_act  \n",
      "19            1.0             0.0      0.6       3    4.0  \n",
      "40            1.0             0.0      1.0       0    4.0  \n",
      "60            1.0             0.0      1.0       2    4.0  \n",
      "61            1.0             0.0      1.0       0    4.0  \n",
      "65            0.0             0.0      2.8       1    2.0  \n",
      "70            1.0             0.0      0.8       0    4.0  \n",
      "76            1.0             0.0      0.4       3    4.0  \n",
      "78            0.0             0.0      2.6       3    2.0  \n",
      "87            0.0             0.0      1.0       2    3.0  \n",
      "106           0.0             1.0      3.4       1    4.0  \n",
      "119           0.0             1.0      6.0       1    3.0  \n",
      "124           0.0             1.0      1.0       2    4.0  \n",
      "126           0.0             1.0      1.0       1    3.0  \n",
      "131           0.0             0.0      0.6       2    3.0  \n",
      "148           0.0             0.0      3.6       2    3.0  \n",
      "149           0.0             0.0      0.8       2    1.0  \n",
      "160           1.0             0.0      1.0       0    4.0  \n",
      "167           0.0             0.0      1.0       2    4.0  \n",
      "172           1.0             0.0      0.8       0    4.0  \n",
      "173           0.0             0.0      1.0       2    4.0  \n",
      "188           0.0             0.0      3.8       2    1.0  \n",
      "192           1.0             0.0      1.0       4    2.0  \n",
      "198           1.0             0.0      1.0       4    0.0  \n",
      "227           1.0             0.0      0.8       4    0.0  \n",
      "235           1.0             0.0      1.0       2    0.0  \n",
      "236           0.0             0.0      5.6       2    1.0  \n",
      "241           0.0             0.0      1.0       4    1.0  \n",
      "253           1.0             0.0      1.0       3    4.0  \n",
      "259           0.0             0.0      2.4       1    4.0  \n",
      "262           1.0             0.0      1.0       0    3.0  \n",
      "...           ...             ...      ...     ...    ...  \n",
      "1494          1.0             0.0      1.0       4    0.0  \n",
      "1507          1.0             0.0      1.0       0    4.0  \n",
      "1514          0.0             0.0      1.0       3    2.0  \n",
      "1549          0.0             0.0      0.8       2    1.0  \n",
      "1550          1.0             0.0      0.6       4    2.0  \n",
      "1562          1.0             0.0      1.0       0    4.0  \n",
      "1587          0.0             0.0      1.0       2    3.0  \n",
      "1588          1.0             0.0      1.0       0    4.0  \n",
      "1590          1.0             0.0      1.0       4    0.0  \n",
      "1598          0.0             0.0      0.8       2    4.0  \n",
      "1603          0.0             0.0      0.6       1    2.0  \n",
      "1638          1.0             0.0      1.0       0    2.0  \n",
      "1643          1.0             0.0      1.0       4    0.0  \n",
      "1653          1.0             0.0      0.4       2    3.0  \n",
      "1658          0.0             0.0      1.6       3    2.0  \n",
      "1680          1.0             0.0      1.0       4    0.0  \n",
      "1700          1.0             0.0      1.0       2    0.0  \n",
      "1703          1.0             0.0      1.0       0    4.0  \n",
      "1710          1.0             0.0      0.8       4    0.0  \n",
      "1711          1.0             0.0      0.8       0    4.0  \n",
      "1753          1.0             0.0      0.8       4    0.0  \n",
      "1769          0.0             0.0      1.8       2    0.0  \n",
      "1776          1.0             0.0      1.0       4    0.0  \n",
      "1784          1.0             0.0      1.0       4    2.0  \n",
      "1791          1.0             0.0      1.0       0    3.0  \n",
      "1811          1.0             0.0      2.6       2    4.0  \n",
      "1816          1.0             0.0      1.0       0    2.0  \n",
      "1819          0.0             0.0      2.0       0    4.0  \n",
      "1822          0.0             1.0      1.0       2    4.0  \n",
      "1827          0.0             1.0      1.8       1    2.0  \n",
      "\n",
      "[206 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "print(newdf)\n",
    "newdf.to_csv('diff_cnn.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
