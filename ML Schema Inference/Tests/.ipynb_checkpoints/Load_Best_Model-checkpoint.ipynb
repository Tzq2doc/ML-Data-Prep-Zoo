{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copyright 2019 Vraj Shah, Arun Kumar\n",
    "#\n",
    "#Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#you may not use this file except in compliance with the License.\n",
    "#You may obtain a copy of the License at\n",
    "#\n",
    "#    http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#Unless required by applicable law or agreed to in writing, software\n",
    "#distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#See the License for the specific language governing permissions and\n",
    "#limitations under the License.\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "np.random.seed(512)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 128\n",
    "maxlen = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read csv\n",
    "dict_label = {'Usable directly numeric':0, 'Usable with extraction':1, 'Usable with Extration': 1, 'Usable with extraction ':1, 'Usable directly categorical':2, 'Unusable':3, 'Context_specific':4, 'Usable directly categorical ':2}\n",
    "data = pd.read_csv('data_for_ML_num.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]\n",
    "data_LSTM = pd.concat([data['Attribute_name'], data['sample_1'], data['sample_2'], data['sample_3'], data['sample_4'], data['sample_5']], axis =1)\n",
    "\n",
    "# X_train, X_test,y_train,y_test = train_test_split(data,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Num of nans'] = [data['Num of nans'][i]*100/data['Total_val'][i] for i in data.index]\n",
    "data['num of dist_val'] = [data['num of dist_val'][i]*100/data['Total_val'][i] for i in data.index]\n",
    "\n",
    "data1 = data[['Num of nans', 'max_val', 'mean', 'min_val', 'num of dist_val','std_dev','castability','extractability', 'len_val']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "# data = data.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev','len_val': 'scaled_len_val'})\n",
    "# column_names_to_normalize = ['scaled_max_val', 'scaled_mean', 'scaled_min_val','scaled_std_dev', 'scaled_len_val']\n",
    "# x = data[column_names_to_normalize].values\n",
    "# x = np.nan_to_num(x)\n",
    "# x_scaled = StandardScaler().fit_transform(x)\n",
    "# df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data.index)\n",
    "# data[column_names_to_normalize] = df_temp\n",
    "\n",
    "# X_train, X_test,y_train,y_test = train_test_split(data,y, test_size=0.2)\n",
    "\n",
    "# new_data = data[['scaled_min_val']]\n",
    "# new_data.scaled_min_val = new_data.scaled_min_val.astype(float)\n",
    "# normalized_df=(new_data-new_data.mean())/new_data.std()\n",
    "# new_data = new_data[np.abs(new_data.scaled_min_val-new_data.scaled_min_val.mean()) <= (3*new_data.scaled_min_val.std())]\n",
    "\n",
    "# for index, row in new_data.iterrows():\n",
    "#     if row['scaled_min_val'] > 100000:\n",
    "#         new_data.iloc[index]['scaled_min_val'] = 100000\n",
    "#     if row['scaled_min_val'] < -100000:\n",
    "#         new_data.iloc[index]['scaled_min_val'] = -100000        \n",
    "#     print(row['scaled_min_val']) \n",
    "\n",
    "# q = new_data['scaled_min_val'].quantile(0.99)\n",
    "# new_data = new_data[new_data['scaled_min_val'] < q]\n",
    "# print(new_data)\n",
    "\n",
    "# print(new_data.mean())\n",
    "# print(new_data.std())\n",
    "\n",
    "\n",
    "# scaled_features = StandardScaler().fit_transform(new_data.values)\n",
    "# df = pd.DataFrame(scaled_features)\n",
    "# scaled_features = MinMaxScaler().fit_transform(new_data.values)\n",
    "# df = pd.DataFrame(scaled_features)\n",
    "\n",
    "# std_scale = preprocessing.StandardScaler().fit(data[['max_val', 'min_val']])\n",
    "# data = std_scale.transform(data[['max_val', 'min_val']])\n",
    "# data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1.to_csv('before.csv')\n",
    "# f = open('current.txt','w')\n",
    "d = enchant.Dict(\"en_US\")\n",
    "\n",
    "for i in data.index:\n",
    "    ival = data.at[i,'Attribute_name']\n",
    "    if ival != 'id' and d.check(ival):\n",
    "#         print >> f,ival\n",
    "#         print >> f,y.at[i,'y_act']\n",
    "        data1.at[i,'dictionary_item'] = 1\n",
    "    else:\n",
    "        data1.at[i,'dictionary_item'] = 0\n",
    "\n",
    "# data1.to_csv('after.csv')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6360\n"
     ]
    }
   ],
   "source": [
    "# print(data1.columns)\n",
    "\n",
    "arr = data['Attribute_name'].values\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(3,3),analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "# print(X.toarray())\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "\n",
    "data2 = pd.concat([data1,tempdf], axis=1, sort=False)\n",
    "\n",
    "# data2.to_csv('after.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8935710426796326\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(data2,y, test_size=0.2,random_state=100)\n",
    "\n",
    "import pickle\n",
    "filename = 'finalized_model.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
