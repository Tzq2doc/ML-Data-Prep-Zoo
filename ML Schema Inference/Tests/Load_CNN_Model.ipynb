{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Embedding, Input, Flatten, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPool1D, Dropout, concatenate,GlobalMaxPooling1D\n",
    "from keras.preprocessing import text as keras_text, sequence as keras_seq\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "np.random.seed(512)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from keras.layers import LeakyReLU, BatchNormalization\n",
    "from keras import initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical\n",
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define network parameters\n",
    "max_features = 128\n",
    "maxlen = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/IPython/core/interactiveshell.py:2714: DtypeWarning: Columns (2,5,10,11,12,13,14,15,17) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#read csv\n",
    "dict_label = {'Usable directly numeric':0, 'Usable with extraction':1, 'Usable with Extration': 1, 'Usable with extraction ':1, 'Usable directly categorical':2, 'Unusable':3, 'Context_specific':4, 'Usable directly categorical ':2}\n",
    "data = pd.read_csv('data_for_ML_num.csv')\n",
    "data['y_act'] = [dict_label[i] for i in data['y_act']]\n",
    "y = data.loc[:,['y_act']]\n",
    "data_LSTM = pd.concat([data['Attribute_name'], data['sample_1'], data['sample_2'], data['sample_3'], data['sample_4'], data['sample_5']], axis =1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hghj\n",
      "hghj\n"
     ]
    }
   ],
   "source": [
    "data = data.rename(columns={'Num of nans': 'Num_of_nans', 'num of dist_val': 'num_of_dist_val'})\n",
    "\n",
    "# data['Num_of_nans'] = [float(data['Num_of_nans'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "# data['num_of_dist_val'] = [float(data['num_of_dist_val'][i])/float(data['Total_val'][i]) for i in data.index]\n",
    "\n",
    "data1 = data[['Num_of_nans', 'max_val', 'mean', 'min_val', 'num_of_dist_val','std_dev','castability','extractability', 'len_val','Total_val']]\n",
    "data1 = data1.fillna(0)\n",
    "\n",
    "data1['Num_of_nans'] = [data1['Num_of_nans'][i]*100/data1['Total_val'][i] for i in data1.index]\n",
    "data1['num_of_dist_val'] = [(data1['num_of_dist_val'][i]*1.0)/data1['Total_val'][i] for i in data1.index]\n",
    "\n",
    "print('hghj')\n",
    "data1 = data1.rename(columns={'mean': 'scaled_mean', 'min_val': 'scaled_min_val', 'max_val': 'scaled_max_val','std_dev': 'scaled_std_dev'})\n",
    "data1.loc[data1['scaled_min_val'] > 10000, 'scaled_min_val'] = 10000\n",
    "data1.loc[data1['scaled_min_val'] < -10000, 'scaled_min_val'] = -10000\n",
    "data1.loc[data1['scaled_max_val'] > 10000, 'scaled_max_val'] = 10000\n",
    "data1.loc[data1['scaled_max_val'] < -10000, 'scaled_max_val'] = -10000\n",
    "data1.loc[data1['scaled_mean'] > 10000, 'scaled_mean'] = 10000\n",
    "data1.loc[data1['scaled_mean'] < -10000, 'scaled_mean'] = -10000\n",
    "data1.loc[data1['scaled_std_dev'] > 10000, 'scaled_std_dev'] = 10000\n",
    "data1.loc[data1['scaled_std_dev'] < -10000, 'scaled_std_dev'] = -10000\n",
    "column_names_to_normalize = ['scaled_max_val', 'scaled_mean', 'scaled_min_val','scaled_std_dev','num_of_dist_val','Num_of_nans','Total_val']\n",
    "x = data1[column_names_to_normalize].values\n",
    "x = np.nan_to_num(x)\n",
    "x_scaled = StandardScaler().fit_transform(x)\n",
    "df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = data1.index)\n",
    "data1[column_names_to_normalize] = df_temp\n",
    "\n",
    "data1.Num_of_nans = data1.Num_of_nans.astype(float)\n",
    "data1.num_of_dist_val = data1.num_of_dist_val.astype(float)\n",
    "data1.castability = data1.castability.astype(float)\n",
    "data1.extractability = data1.extractability.astype(float)\n",
    "y.y_act = y.y_act.astype(float)\n",
    "\n",
    "\n",
    "print('hghj')\n",
    "\n",
    "data1 = data1[['Num_of_nans', 'scaled_max_val', 'scaled_mean', 'scaled_min_val', 'num_of_dist_val','scaled_std_dev','castability','extractability', 'len_val','Total_val']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1927\n"
     ]
    }
   ],
   "source": [
    "arr = data['Attribute_name'].values\n",
    "arr1 = data['sample_1'].values\n",
    "arr1 = [str(x) for x in arr1]\n",
    "arr2 = data['sample_2'].values\n",
    "arr2 = [str(x) for x in arr2]\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2),analyzer='char')\n",
    "X = vectorizer.fit_transform(arr)\n",
    "X1 = vectorizer.fit_transform(arr1)\n",
    "X2 = vectorizer.fit_transform(arr2)\n",
    "print(len(vectorizer.get_feature_names()))\n",
    "# print(X.toarray())\n",
    "\n",
    "# data1.to_csv('before.csv')\n",
    "\n",
    "tempdf = pd.DataFrame(X.toarray())\n",
    "tempdf1 = pd.DataFrame(X1.toarray())\n",
    "tempdf2 = pd.DataFrame(X2.toarray())\n",
    "\n",
    "data2 = pd.concat([data1,tempdf,tempdf1,tempdf2], axis=1, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n",
      "7400\n",
      "5122\n",
      "7400\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test,y_train,y_test = train_test_split(data2,y, test_size=0.2,random_state=100)\n",
    "\n",
    "key_name = data['Attribute_name']\n",
    "atr_train,atr_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "\n",
    "\n",
    "X_train = X_train.reset_index(drop=True)\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "# atr_train.reset_index(inplace=True,drop=True)\n",
    "# atr_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "print(len(X_train))\n",
    "# print(atr_train)\n",
    "list_sentences_train = atr_train.values\n",
    "# list_sentences_train = atr_trainv\n",
    "list_sentences_test = atr_test.values\n",
    "# print(list_sentences_train)\n",
    "\n",
    "structured_data_train = X_train \n",
    "structured_data_test = X_test\n",
    "\n",
    "print(len(structured_data_train))\n",
    "print(len(list(structured_data_train.loc[2])))\n",
    "\n",
    "# structured_input_train = []\n",
    "# for i in range(len(structured_data_train)):\n",
    "#     if i%100 == 0:\n",
    "#         print(i)\n",
    "#     structured_input_train.append(list(structured_data_train.loc[i]))\n",
    "# structured_input_train = np.array(structured_input_train).reshape(len(structured_data_train),len(structured_data_train.keys()))    \n",
    "\n",
    "# structured_input_test = []\n",
    "# for i in range(len(structured_data_train)):\n",
    "#     structured_input_test.append(list(structured_data_train.loc[i]))\n",
    "# structured_input_test = np.array(structured_input_test).reshape(len(structured_data_train),len(structured_data_train.keys()))    \n",
    "\n",
    "print(len(structured_data_train))\n",
    "print(len(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_name = data['sample_1']\n",
    "samp1_train,samp1_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "samp1_train.reset_index(inplace=True,drop=True)\n",
    "samp1_test.reset_index(inplace=True,drop=True)\n",
    "\n",
    "\n",
    "key_name = data['sample_2']\n",
    "samp2_train,samp2_test = train_test_split(key_name, test_size=0.2,random_state=100)\n",
    "samp2_train.reset_index(inplace=True,drop=True)\n",
    "samp2_test.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7400\n",
      "['#NULL!' '0' '268' ... 'Hate' '-0.101' '2']\n",
      "['0' '78' '110' ... nan '0.171' '5']\n",
      "7400\n"
     ]
    }
   ],
   "source": [
    "\n",
    "list_sentences_train = atr_train.values\n",
    "list_sentences_test = atr_test.values\n",
    "\n",
    "print(len(list_sentences_train))\n",
    "\n",
    "# X_train.sample_1 = X_train.sample_1.astype(str)\n",
    "# X_test.sample_1 = X_test.sample_1.astype(str)\n",
    "\n",
    "list_sentences_train1 = samp1_train.values\n",
    "list_sentences_test1 = samp1_test.values\n",
    "\n",
    "print(list_sentences_train1)\n",
    "\n",
    "# X_train.sample_2 = X_train.sample_2.astype(str)\n",
    "# X_test.sample_2 = X_test.sample_2.astype(str)\n",
    "\n",
    "list_sentences_train2 = samp2_train.values\n",
    "list_sentences_test2 = samp2_test.values\n",
    "\n",
    "print(list_sentences_train2)\n",
    "\n",
    "for i in range(len(list_sentences_train)):\n",
    "    list_sentences_train[i] = str(list_sentences_train[i])\n",
    "    list_sentences_train1[i] = str(list_sentences_train1[i])\n",
    "    list_sentences_train2[i] = str(list_sentences_train2[i])\n",
    "    \n",
    "for i in range(len(list_sentences_test)):\n",
    "    list_sentences_test[i] = str(list_sentences_test[i])\n",
    "    list_sentences_test1[i] = str(list_sentences_test1[i])\n",
    "    list_sentences_test2[i] = str(list_sentences_test2[i])    \n",
    "\n",
    "print(len(list_sentences_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "# train data\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "X_t = keras_seq.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_te = keras_seq.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "\n",
    "tokenizer1 = keras_text.Tokenizer(char_level = True)\n",
    "tokenizer1.fit_on_texts(list(list_sentences_train1))\n",
    "# train data\n",
    "list_tokenized_train1 = tokenizer.texts_to_sequences(list_sentences_train1)\n",
    "X_t1 = keras_seq.pad_sequences(list_tokenized_train1, maxlen=maxlen)\n",
    "# test data\n",
    "list_tokenized_test1 = tokenizer.texts_to_sequences(list_sentences_test1)\n",
    "X_te1 = keras_seq.pad_sequences(list_tokenized_test1, maxlen=maxlen)\n",
    "\n",
    "\n",
    "# tokenizer2 = keras_text.Tokenizer(char_level = True)\n",
    "# tokenizer2.fit_on_texts(list(list_sentences_train1))\n",
    "# # train data\n",
    "# list_tokenized_train2 = tokenizer.texts_to_sequences(list_sentences_train2)\n",
    "# X_t2 = keras_seq.pad_sequences(list_tokenized_train2, maxlen=maxlen)\n",
    "# # test data\n",
    "# list_tokenized_test2 = tokenizer.texts_to_sequences(list_sentences_test2)\n",
    "# X_te2 = keras_seq.pad_sequences(list_tokenized_test2, maxlen=maxlen)\n",
    "\n",
    "# print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "7400/7400 [==============================] - 1s 140us/step\n",
      "(0.14884592010765463, 0.9597297297297297)\n",
      "1851/1851 [==============================] - 0s 87us/step\n",
      "(0.4580955585391376, 0.8816855751392585)\n",
      "---\n",
      "7400/7400 [==============================] - 1s 148us/step\n",
      "(0.1644142657493216, 0.9675675675675676)\n",
      "1851/1851 [==============================] - 0s 87us/step\n",
      "(0.5964181145772878, 0.8849270665471712)\n",
      "---\n",
      "7400/7400 [==============================] - 1s 139us/step\n",
      "(0.1576258554974118, 0.9577027027027027)\n",
      "1851/1851 [==============================] - 0s 90us/step\n",
      "(0.5255573344578426, 0.8827660728119181)\n",
      "---\n",
      "7400/7400 [==============================] - 1s 143us/step\n",
      "(0.15438254611113586, 0.9625675675675676)\n",
      "1851/1851 [==============================] - 0s 84us/step\n",
      "(0.4693830701578378, 0.8789843326642796)\n",
      "---\n",
      "7400/7400 [==============================] - 1s 147us/step\n",
      "(0.153708900954524, 0.9633783783783784)\n",
      "1851/1851 [==============================] - 0s 86us/step\n",
      "(0.5525369347751882, 0.8816855757832849)\n",
      "0.8820097245891825\n"
     ]
    }
   ],
   "source": [
    "k=5\n",
    "i=0\n",
    "avg_test = 0\n",
    "for i in range(5):\n",
    "    bestone = load_model('best_weights'+str(i)+'.h5')\n",
    "    print('---')\n",
    "    loss, acc = bestone.evaluate([X_t,structured_data_train],to_categorical(y_train),verbose=1)\n",
    "    print(loss, acc)\n",
    "    loss, acc = bestone.evaluate([X_te,structured_data_test],to_categorical(y_test),verbose=1)\n",
    "    print(loss, acc)\n",
    "    avg_test = avg_test + acc\n",
    "\n",
    "print(avg_test/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
